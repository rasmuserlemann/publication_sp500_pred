{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "âŒ Failed to fetch SPY data from Alpha Vantage.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Fetch SPY data\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m spy \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_spy_alpha_vantage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAPI_KEY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Print available data range\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst available date: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspy\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [14], line 20\u001b[0m, in \u001b[0;36mfetch_spy_alpha_vantage\u001b[0;34m(api_key)\u001b[0m\n\u001b[1;32m     18\u001b[0m data \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime Series (Daily)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ Failed to fetch SPY data from Alpha Vantage.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime Series (Daily)\u001b[39m\u001b[38;5;124m\"\u001b[39m], orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1. open\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopen\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2. high\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6. volume\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     30\u001b[0m })\n",
      "\u001b[0;31mValueError\u001b[0m: âŒ Failed to fetch SPY data from Alpha Vantage."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# Your Alpha Vantage API Key\n",
    "API_KEY = \"6GMUD3MXU7HKTDSY\"\n",
    "\n",
    "# Fetch SPY daily data from Alpha Vantage\n",
    "def fetch_spy_alpha_vantage(api_key):\n",
    "    url = f\"https://www.alphavantage.co/query\"\n",
    "    params = {\n",
    "        \"function\": \"TIME_SERIES_DAILY_ADJUSTED\",\n",
    "        \"symbol\": \"SPY\",\n",
    "        \"outputsize\": \"full\",\n",
    "        \"apikey\": api_key\n",
    "    }\n",
    "    r = requests.get(url, params=params)\n",
    "    data = r.json()\n",
    "    if \"Time Series (Daily)\" not in data:\n",
    "        raise ValueError(\"âŒ Failed to fetch SPY data from Alpha Vantage.\")\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data[\"Time Series (Daily)\"], orient=\"index\")\n",
    "    df = df.rename(columns={\n",
    "        '1. open': 'open',\n",
    "        '2. high': 'high',\n",
    "        '3. low': 'low',\n",
    "        '4. close': 'close',\n",
    "        '5. adjusted close': 'adjusted_close',\n",
    "        '6. volume': 'volume'\n",
    "    })\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df = df.sort_index()\n",
    "    df = df[['adjusted_close']]\n",
    "    return df\n",
    "\n",
    "# Fetch SPY data\n",
    "spy = fetch_spy_alpha_vantage(API_KEY)\n",
    "\n",
    "# Print available data range\n",
    "print(f\"First available date: {spy.index.min().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Last available date: {spy.index.max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Filter for target range if possible\n",
    "spy = spy.loc[\"1993-01-29\":\"2024-12-30\"]\n",
    "\n",
    "# Step 2: Calculate daily returns\n",
    "spy['Prev_Close'] = spy['adjusted_close'].shift(1)\n",
    "spy['Daily_Return'] = (spy['adjusted_close'] - spy['Prev_Close']) / spy['Prev_Close']\n",
    "\n",
    "# Step 3: Strategy: Only hold if previous day was positive\n",
    "spy['Prev_Return'] = spy['Daily_Return'].shift(1)\n",
    "spy['Strategy_Return'] = np.where(spy['Prev_Return'] > 0, spy['Daily_Return'], 0)\n",
    "\n",
    "# Step 4: Calculate cumulative returns\n",
    "spy['Cumulative_Strategy_Return'] = (1 + spy['Strategy_Return']).cumprod()\n",
    "\n",
    "# Step 5: Calculate total and annualized return\n",
    "total_days = (spy.index[-1] - spy.index[0]).days\n",
    "total_years = total_days / 365.25\n",
    "\n",
    "final_cumulative_return = spy['Cumulative_Strategy_Return'].iloc[-1]\n",
    "average_annual_return = (final_cumulative_return) ** (1/total_years) - 1\n",
    "\n",
    "# Output results\n",
    "print(f\"âœ… Final cumulative return (strategy): {final_cumulative_return:.2f}x\")\n",
    "print(f\"âœ… Average annual return: {average_annual_return * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading EUR_USD...\n",
      "Downloading USD_JPY...\n",
      "Downloading GBP_USD...\n",
      "Downloading AUD_USD...\n",
      "Downloading USD_CAD...\n",
      "Data saved to data/forex.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import investpy\n",
    "\n",
    "# Define FOREX pairs and their Investing.com names\n",
    "forex_pairs = {\n",
    "    'EUR_USD': 'eur/usd',\n",
    "    'USD_JPY': 'usd/jpy',\n",
    "    'GBP_USD': 'gbp/usd',\n",
    "    'AUD_USD': 'aud/usd',\n",
    "    'USD_CAD': 'usd/cad'\n",
    "}\n",
    "\n",
    "# Define the date range for March 2025\n",
    "start_date = '01/04/2025'  # DD/MM/YYYY\n",
    "end_date = '30/04/2025'\n",
    "\n",
    "# Folder to save data\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Initialize main DataFrame\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Download and format data\n",
    "for pair_name, pair in forex_pairs.items():\n",
    "    print(f\"Downloading {pair_name}...\")\n",
    "    try:\n",
    "        data = investpy.get_currency_cross_historical_data(\n",
    "            currency_cross=pair.upper(),\n",
    "            from_date=start_date,\n",
    "            to_date=end_date\n",
    "        )\n",
    "        data = data[['Open', 'High', 'Low', 'Close']]\n",
    "        data.columns = [\n",
    "            f'open_{pair_name}',\n",
    "            f'high_{pair_name}',\n",
    "            f'low_{pair_name}',\n",
    "            f'close_{pair_name}'\n",
    "        ]\n",
    "        \n",
    "        if combined_df.empty:\n",
    "            combined_df = data.copy()\n",
    "        else:\n",
    "            combined_df = combined_df.join(data, how='outer')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {pair_name}: {e}\")\n",
    "\n",
    "# Final formatting\n",
    "combined_df.index.name = 'timestamp'\n",
    "combined_df = combined_df.reset_index()\n",
    "\n",
    "# Save to CSV\n",
    "output_path = os.path.join('data', 'forex.csv')\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIX - investing.com\n",
    "\n",
    "SPGSCI - investing.com\n",
    "\n",
    "BTC - investing.com\n",
    "\n",
    "Copper - https://www.macrotrends.net/1476/copper-prices-historical-chart-data\n",
    "\n",
    "Oil - https://www.macrotrends.net/2516/wti-crude-oil-prices-10-year-daily-chart\n",
    "\n",
    "MOEX - https://www.investing.com/indices/mcx-historical-data?utm_source=chatgpt.com\n",
    "\n",
    "STOXX 600 - https://www.investing.com/indices/stoxx-600?utm_source=chatgpt.com\n",
    "\n",
    "SSE - https://www.investing.com/indices/shanghai-composite-historical-data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 10-year Treasury rates to data/treasury_10y.csv\n",
      "Data range: 2025-04-01 to 2025-04-30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fredapi import Fred\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def get_treasury_10y_rates(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch US Treasury 10-year daily rates from FRED API and save to CSV\n",
    "    \"\"\"\n",
    "    # Initialize FRED API\n",
    "    fred = Fred(api_key='65d4ac185aed89dd59c0f551a6db85b0')\n",
    "    \n",
    "    # Series ID for 10-Year Treasury Constant Maturity Rate\n",
    "    series_id = 'DGS10'  # FRED code for 10-Year Treasury Rate\n",
    "    \n",
    "    try:\n",
    "        # Create data directory if it doesn't exist\n",
    "        os.makedirs('data', exist_ok=True)\n",
    "        \n",
    "        # Fetch the data\n",
    "        data = fred.get_series(series_id, start_date, end_date)\n",
    "        \n",
    "        if data.empty:\n",
    "            current_year = datetime.now().year\n",
    "            print(f\"No data available for {start_date} to {end_date}\")\n",
    "            if int(start_date[:4]) > current_year:\n",
    "                print(f\"Note: Future data (April {start_date[:4]}) isn't available yet\")\n",
    "            return False\n",
    "            \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(data, columns=['Rate'])\n",
    "        df.index.name = 'Date'\n",
    "        df['Rate'] = df['Rate'].astype(float)\n",
    "        \n",
    "        # Save to CSV\n",
    "        csv_path = 'data/treasury_10y.csv'\n",
    "        df.to_csv(csv_path)\n",
    "        print(f\"Successfully saved 10-year Treasury rates to {csv_path}\")\n",
    "        print(f\"Data range: {df.index[0].date()} to {df.index[-1].date()}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return False\n",
    "\n",
    "# Define date range for April 2025\n",
    "start_date = '2025-04-01'\n",
    "end_date = '2025-04-30'\n",
    "\n",
    "# Get and save the data\n",
    "success = get_treasury_10y_rates(start_date, end_date)\n",
    "\n",
    "if not success:\n",
    "    # Example fallback to available data\n",
    "    print(\"\\nTrying available historical data for demonstration...\")\n",
    "    get_treasury_10y_rates('2024-04-01', '2024-04-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¡ Fetching Bitcoin historical data...\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "âŒ API error or rate limit. Response was: {'error': {'status': {'timestamp': '2025-05-04T17:27:37.840+00:00', 'error_code': 10012, 'error_message': 'Your request exceeds the allowed time range. Public API users are limited to querying historical data within the past 365 days. Upgrade to a paid plan to enjoy full historical data access: https://www.coingecko.com/en/api/pricing. '}}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [52], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Check for errors in response\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprices\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_volumes\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ API error or rate limit. Response was: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Convert price data\u001b[39;00m\n\u001b[1;32m     29\u001b[0m prices \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprices\u001b[39m\u001b[38;5;124m'\u001b[39m], columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mException\u001b[0m: âŒ API error or rate limit. Response was: {'error': {'status': {'timestamp': '2025-05-04T17:27:37.840+00:00', 'error_code': 10012, 'error_message': 'Your request exceeds the allowed time range. Public API users are limited to querying historical data within the past 365 days. Upgrade to a paid plan to enjoy full historical data access: https://www.coingecko.com/en/api/pricing. '}}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "# Configuration\n",
    "START_DATE = \"2025-04-01\"\n",
    "END_DATE = (datetime.today() - BDay(1)).strftime('%Y-%m-%d')\n",
    "OUTPUT_FILE = \"data/bitcoin.csv\"\n",
    "COINGECKO_URL = (\n",
    "    \"https://api.coingecko.com/api/v3/coins/bitcoin/market_chart\"\n",
    "    \"?vs_currency=usd&days=max&interval=daily\"\n",
    ")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "# Request data\n",
    "print(\"ðŸ“¡ Fetching Bitcoin historical data...\")\n",
    "response = requests.get(COINGECKO_URL)\n",
    "data = response.json()\n",
    "\n",
    "# Check for errors in response\n",
    "if 'prices' not in data or 'total_volumes' not in data:\n",
    "    raise Exception(f\"âŒ API error or rate limit. Response was: {data}\")\n",
    "\n",
    "# Convert price data\n",
    "prices = pd.DataFrame(data['prices'], columns=['timestamp', 'Close'])\n",
    "prices['timestamp'] = pd.to_datetime(prices['timestamp'], unit='ms').dt.floor('D')\n",
    "\n",
    "# Use Close as proxy for OHLC\n",
    "prices['Open'] = prices['Close']\n",
    "prices['High'] = prices['Close']\n",
    "prices['Low'] = prices['Close']\n",
    "\n",
    "# Convert volume data\n",
    "volumes = pd.DataFrame(data['total_volumes'], columns=['timestamp', 'Volume'])\n",
    "volumes['timestamp'] = pd.to_datetime(volumes['timestamp'], unit='ms').dt.floor('D')\n",
    "\n",
    "# Merge\n",
    "df = prices.merge(volumes, on='timestamp', how='left')\n",
    "\n",
    "# Filter date range\n",
    "df = df[(df['timestamp'] >= START_DATE) & (df['timestamp'] <= END_DATE)]\n",
    "\n",
    "# Format and save\n",
    "df = df[['timestamp', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "df[['Open', 'High', 'Low', 'Close']] = df[['Open', 'High', 'Low', 'Close']].round(2)\n",
    "df['Volume'] = df['Volume'].round(2)\n",
    "df['timestamp'] = df['timestamp'].dt.strftime('%Y-%m-%d')\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"âœ… Bitcoin data saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Available columns: ['Date', 'Open', 'High', 'Low', 'Close Close price adjusted for splits.', 'Adj Close Adjusted close price adjusted for splits and dividend and/or capital gain distributions.', 'Volume']\n",
      "âœ… S&P 500 data scraped and saved to data/sp500.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sd/x6b24fj17sg_pd8tj67xfp4r0000gn/T/ipykernel_18014/3001098383.py:30: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(tables[0]))[0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "URL = \"https://finance.yahoo.com/quote/%5EGSPC/history\"\n",
    "START_DATE = \"2025-04-01\"\n",
    "OUTPUT_FILE = \"data/sp500.csv\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "# Request page content with headers\n",
    "response = requests.get(URL, headers=HEADERS)\n",
    "if response.status_code == 429:\n",
    "    raise Exception(\"âŒ HTTP 429: Too Many Requests. You're being rate-limited by Yahoo.\")\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "tables = soup.find_all(\"table\")\n",
    "\n",
    "if not tables:\n",
    "    raise Exception(\"âŒ No table found. Yahoo Finance layout may have changed.\")\n",
    "\n",
    "# Parse first table into DataFrame\n",
    "df = pd.read_html(str(tables[0]))[0]\n",
    "\n",
    "# Debug: Print column names to confirm structure\n",
    "print(\"ðŸ“‹ Available columns:\", list(df.columns))\n",
    "\n",
    "# Try to find the closest match to the 'Close' column\n",
    "close_column = next((col for col in df.columns if \"Close\" in col), None)\n",
    "if close_column is None:\n",
    "    raise Exception(\"âŒ 'Close' column not found in table. Yahoo layout likely changed.\")\n",
    "\n",
    "# Drop rows without numeric Close prices (e.g., 'Dividend', 'Split', NaN)\n",
    "df = df[df[close_column].apply(lambda x: isinstance(x, (int, float)) or str(x).replace('.', '', 1).isdigit())]\n",
    "\n",
    "# Convert and clean dates\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='mixed', errors='coerce')\n",
    "df = df[df['Date'] >= pd.to_datetime(START_DATE)]\n",
    "\n",
    "# Rename and format\n",
    "df = df.rename(columns={\n",
    "    'Date': 'timestamp',\n",
    "    'Open': 'Open',\n",
    "    'High': 'High',\n",
    "    'Low': 'Low',\n",
    "    close_column: 'Close',\n",
    "    'Volume': 'Volume'\n",
    "})\n",
    "df = df[['timestamp', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "df[['Open', 'High', 'Low', 'Close']] = df[['Open', 'High', 'Low', 'Close']].astype(float).round(2)\n",
    "df['Volume'] = df['Volume'].astype(str).str.replace(',', '').astype(float)\n",
    "df['timestamp'] = df['timestamp'].dt.strftime('%Y-%m-%d')\n",
    "df = df.sort_values('timestamp')\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"âœ… S&P 500 data scraped and saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Available columns: ['Date', 'Open', 'High', 'Low', 'Close Close price adjusted for splits.', 'Adj Close Adjusted close price adjusted for splits and dividend and/or capital gain distributions.', 'Volume']\n",
      "âœ… Oil price data saved to data/oil.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sd/x6b24fj17sg_pd8tj67xfp4r0000gn/T/ipykernel_18014/2462068041.py:34: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(tables[0]))[0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "URL = \"https://finance.yahoo.com/quote/CL%3DF/history\"\n",
    "START_DATE = \"2025-04-01\"\n",
    "END_DATE = \"2025-04-30\"\n",
    "OUTPUT_FILE = \"data/oil.csv\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "# Request page content with headers\n",
    "response = requests.get(URL, headers=HEADERS)\n",
    "if response.status_code == 429:\n",
    "    raise Exception(\"âŒ HTTP 429: Too Many Requests. You're being rate-limited by Yahoo.\")\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"âŒ HTTP {response.status_code}: Failed to load data.\")\n",
    "\n",
    "# Parse table using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "tables = soup.find_all(\"table\")\n",
    "\n",
    "if not tables:\n",
    "    raise Exception(\"âŒ No table found. Yahoo Finance layout may have changed.\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.read_html(str(tables[0]))[0]\n",
    "\n",
    "# Print column names for verification\n",
    "print(\"ðŸ“‹ Available columns:\", list(df.columns))\n",
    "\n",
    "# Find the column that contains 'Close'\n",
    "close_column = next((col for col in df.columns if \"Close\" in col), None)\n",
    "if close_column is None:\n",
    "    raise Exception(\"âŒ 'Close' column not found. Layout likely changed.\")\n",
    "\n",
    "# Remove non-price rows (e.g., dividends, splits)\n",
    "df = df[df[close_column].apply(lambda x: isinstance(x, (int, float)) or str(x).replace('.', '', 1).isdigit())]\n",
    "\n",
    "# Parse and filter dates\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='mixed', errors='coerce')\n",
    "df = df[(df['Date'] >= pd.to_datetime(START_DATE)) & (df['Date'] <= pd.to_datetime(END_DATE))]\n",
    "\n",
    "# Rename and reformat\n",
    "df = df.rename(columns={\n",
    "    'Date': 'timestamp',\n",
    "    'Open': 'Open',\n",
    "    'High': 'High',\n",
    "    'Low': 'Low',\n",
    "    close_column: 'Close',\n",
    "    'Volume': 'Volume'\n",
    "})\n",
    "df = df[['timestamp', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "df[['Open', 'High', 'Low', 'Close']] = df[['Open', 'High', 'Low', 'Close']].astype(float).round(4)\n",
    "df['Volume'] = df['Volume'].astype(str).str.replace(',', '').astype(float)\n",
    "df['timestamp'] = df['timestamp'].dt.strftime('%Y-%m-%d')\n",
    "df = df.sort_values('timestamp')\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"âœ… Oil price data saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Google Trends data saved to data/google_trends.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pytrends.request import TrendReq\n",
    "import os\n",
    "\n",
    "# Set up pytrends\n",
    "pytrends = TrendReq(hl='en-US', tz=360)\n",
    "\n",
    "# Define search terms\n",
    "keywords = ['sp500', 'SPX', 'index fund', 'ETF']\n",
    "start_date = '2025-04-01'\n",
    "end_date = '2025-04-30'\n",
    "geo = ''  # Can set to 'US' for US-specific trends\n",
    "\n",
    "# Build payload\n",
    "pytrends.build_payload(keywords, timeframe=f'{start_date} {end_date}', geo=geo)\n",
    "\n",
    "# Fetch interest over time\n",
    "df = pytrends.interest_over_time()\n",
    "\n",
    "# Drop 'isPartial' column if it exists\n",
    "if 'isPartial' in df.columns:\n",
    "    df = df.drop(columns=['isPartial'])\n",
    "\n",
    "# Compute 7-day rolling average\n",
    "df_rolling = df.rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "# Reset index and rename for CSV output\n",
    "df_rolling = df_rolling.reset_index()\n",
    "df_rolling.rename(columns={'date': 'timestamp'}, inplace=True)\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "df_rolling.to_csv('data/google_trends.csv', index=False)\n",
    "\n",
    "print(\"âœ… Google Trends data saved to data/google_trends.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
